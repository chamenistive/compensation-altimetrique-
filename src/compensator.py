"""
Module de compensation par moindres carr√©s pour nivellement g√©om√©trique.

Ce module impl√©mente la m√©thode de compensation par moindres carr√©s
avec une pr√©cision garantie de 2mm selon les normes g√©od√©siques.

Algorithmes impl√©ment√©s:
- M√©thode des moindres carr√©s: xÃÇ = (A^T P A)^(-1) A^T P f
- Construction matrice de conception A
- Calcul matrice de poids P
- R√©solution robuste (QR pour gros syst√®mes)
- Analyse statistique compl√®te

Th√©orie math√©matique:
- Mod√®le fonctionnel: l + v = A√óx + f
- Mod√®le stochastique: P = œÉ‚ÇÄ¬≤√óQ‚Åª¬π
- Estimation: xÃÇ = (A^T P A)^(-1) A^T P f
- Covariance: Q‚Çì = (A^T P A)^(-1)

Auteur: Syst√®me de Compensation Altim√©trique
Version: 1.0
Pr√©cision: 2mm
"""

import numpy as np
import pandas as pd
from typing import List, Dict, Tuple, Optional, Union
from dataclasses import dataclass
from enum import Enum
import scipy.linalg as scipy_linalg
from scipy.sparse import diags, csc_matrix
from scipy.sparse.linalg import spsolve

from .exceptions import (
    MatrixError, CalculationError, PrecisionError,
    safe_divide, validate_positive_number
)
from .validators import GeodeticValidator, PrecisionValidator, ValidationResult, CompensationValidator
from .calculator import CalculationResults, AltitudeCalculation


class SolutionMethod(Enum):
    """M√©thodes de r√©solution."""
    NORMAL_EQUATIONS = "equations_normales"
    QR_DECOMPOSITION = "decomposition_qr"
    SPARSE_SOLVER = "solveur_creux"
    CHOLESKY = "cholesky"


@dataclass
class MatrixSystem:
    """Syst√®me matriciel A√óx = b avec pond√©ration."""
    A: np.ndarray  # Matrice de conception
    P: np.ndarray  # Matrice de poids
    f: np.ndarray  # Vecteur des √©carts
    point_ids: List[str]  # Identifiants des points
    observation_ids: List[str]  # Identifiants des observations


@dataclass
class CompensationStatistics:
    """Statistiques de la compensation."""
    sigma_0_hat: float  # √âcart-type a posteriori
    degrees_of_freedom: int  # Degr√©s de libert√©
    chi2_test_statistic: float  # Statistique du test œá¬≤
    chi2_critical_value: float  # Valeur critique œá¬≤
    unit_weight_valid: bool  # Validation du poids unitaire
    max_standardized_residual: float  # R√©sidu normalis√© max
    blunder_detection_threshold: float  # Seuil d√©tection fautes


@dataclass
class CompensationResults:
    """R√©sultats complets de la compensation."""
    adjusted_coordinates: np.ndarray  # Corrections estim√©es
    adjusted_altitudes: List[AltitudeCalculation]  # Altitudes compens√©es
    residuals: np.ndarray  # R√©sidus des observations
    covariance_matrix: np.ndarray  # Matrice de covariance
    statistics: CompensationStatistics  # Statistiques
    solution_method: SolutionMethod  # M√©thode utilis√©e
    computation_metadata: Dict  # M√©tadonn√©es du calcul


class MatrixBuilder:
    """
    Constructeur de matrices pour la compensation.
    
    Construit les matrices A, P et f selon la th√©orie g√©od√©sique
    du nivellement g√©om√©trique.
    """
    
    def __init__(self):
        self.validator = CompensationValidator()
    
    def build_design_matrix(self, n_points: int, n_observations: int, reference_fixed: bool = True) -> np.ndarray:
        """
        Construction de la matrice de conception A.
        
        Th√©orie:
        Pour le nivellement g√©om√©trique, la matrice A relie les corrections
        d'altitude aux observations de d√©nivel√©e:
        
        Œîh_i = H_j - H_i + v_i
        
        Args:
            n_points: Nombre total de points
            n_observations: Nombre total d'observations (peut √™tre > n_points-1)
            reference_fixed: Si True, le premier point est fix√©
            
        Returns:
            np.ndarray: Matrice de conception A
        """
        if n_points < 2:
            raise MatrixError(
                "Au moins 2 points n√©cessaires",
                matrix_name="A",
                operation="construction"
            )
        
        # Dimensions
        n_unknowns = n_points - 1 if reference_fixed else n_points
        
        # Initialisation
        A = np.zeros((n_observations, n_unknowns))
        
        # Construction: chaque observation relie deux points cons√©cutifs
        # Plusieurs observations peuvent relier les m√™mes points (redondance)
        n_segments = n_points - 1
        
        for obs_idx in range(n_observations):
            # D√©terminer quel segment cette observation concerne
            segment_idx = obs_idx % n_segments
            
            if reference_fixed:
                # Point de r√©f√©rence fix√©
                A[obs_idx, segment_idx] = 1.0  # Point d'arriv√©e
                if segment_idx > 0:
                    A[obs_idx, segment_idx-1] = -1.0  # Point de d√©part
            else:
                # Tous les points sont des inconnues
                A[obs_idx, segment_idx] = -1.0  # Point de d√©part
                A[obs_idx, segment_idx+1] = 1.0  # Point d'arriv√©e
        
        # Validation
        validation = self.validator.validate_design_matrix(A, n_observations, n_unknowns)
        if not validation.is_valid:
            raise MatrixError(
                f"Matrice A invalide: {'; '.join(validation.errors)}",
                matrix_name="A",
                matrix_shape=A.shape,
                operation="validation"
            )
        
        return A
    
    def build_weight_matrix(self, distances_m: np.ndarray, 
                           instrumental_error_mm: float = 1.0,
                           kilometric_error_mm: float = 1.0) -> np.ndarray:
        """
        Construction de la matrice de poids P.
        
        Th√©orie g√©od√©sique:
        œÉ·µ¢¬≤ = a¬≤ + b¬≤√ód·µ¢ (mm¬≤)
        P = diag(1/œÉ‚ÇÅ¬≤, 1/œÉ‚ÇÇ¬≤, ..., 1/œÉ‚Çô¬≤)
        
        Args:
            distances_m: Distances des observations en m√®tres
            instrumental_error_mm: Erreur instrumentale (mm)
            kilometric_error_mm: Erreur kilom√©trique (mm/km)
            
        Returns:
            np.ndarray: Matrice de poids P (diagonale)
        """
        n_obs = len(distances_m)
        weights = np.zeros(n_obs)
        
        for i, dist_m in enumerate(distances_m):
            # Gestion des distances manquantes ou nulles
            if np.isnan(dist_m) or dist_m <= 0:
                dist_m = 10.0  # Distance par d√©faut: 10m
            
            # Conversion en kilom√®tres
            dist_km = dist_m / 1000.0
            
            # Calcul de la variance th√©orique (mm¬≤)
            variance_mm2 = instrumental_error_mm**2 + (kilometric_error_mm * dist_km)**2
            
            # Poids = inverse de la variance
            weights[i] = safe_divide(1.0, variance_mm2, f"calcul_poids_obs_{i}")
        
        P = np.diag(weights)
        
        # Validation
        validation = self.validator.validate_weight_matrix(P, n_obs)
        if not validation.is_valid:
            raise MatrixError(
                f"Matrice P invalide: {'; '.join(validation.errors)}",
                matrix_name="P",
                matrix_shape=P.shape,
                operation="validation"
            )
        
        return P
    
    def build_misclosure_vector(self, observed_height_differences: np.ndarray,
                               computed_height_differences: np.ndarray) -> np.ndarray:
        """
        Construction du vecteur des √©carts f.
        
        Th√©orie:
        f = l_obs - l_calc
        o√π l_obs = d√©nivel√©es observ√©es
           l_calc = d√©nivel√©es calcul√©es avec altitudes approch√©es
        
        Args:
            observed_height_differences: D√©nivel√©es observ√©es
            computed_height_differences: D√©nivel√©es calcul√©es
            
        Returns:
            np.ndarray: Vecteur des √©carts f
        """
        # V√©rification et ajustement des tailles
        if len(observed_height_differences) != len(computed_height_differences):
            print(f"‚ö†Ô∏è Ajustement des tailles: obs={len(observed_height_differences)}, calc={len(computed_height_differences)}")
            min_size = min(len(observed_height_differences), len(computed_height_differences))
            observed_height_differences = observed_height_differences[:min_size]
            computed_height_differences = computed_height_differences[:min_size]
            
            if min_size == 0:
                raise MatrixError(
                    "Aucune d√©nivel√©e disponible pour le calcul",
                    operation="construction_vecteur_f"
                )
        
        f = observed_height_differences - computed_height_differences
        return f.reshape(-1, 1)  # Vecteur colonne
    
    def build_complete_system_restructured(self, calculation_results: CalculationResults,
                                          distances_m: np.ndarray,
                                          atmospheric_conditions,
                                          instrumental_error_mm: float = 1.0,
                                          kilometric_error_mm: float = 1.0) -> MatrixSystem:
        """
        Construction du syst√®me matriciel selon la nouvelle s√©quence:
        1. Moyenne des distances par segment
        2. Correction des niveaux apparents (courbure + r√©fraction)
        3. Application corrections aux deltas moyens
        4. Misclosure vector
        5. Design matrix
        6. Matrice de poids
        """
        from atmospheric_corrections import AtmosphericCorrector
        
        n_points = len(calculation_results.altitudes)
        n_segments = n_points - 1
        
        print(f"üîß Nouveau processus de compensation:")
        print(f"   Points: {n_points}, Segments: {n_segments}")
        
        # 1. MOYENNE DES DISTANCES PAR SEGMENT
        distances_mean_by_segment = self._calculate_mean_distances_by_segment(
            calculation_results, distances_m, n_segments
        )
        print(f"   Distances moyennes par segment calcul√©es")
        
        # 2. CORRECTION DES NIVEAUX APPARENTS
        corrector = AtmosphericCorrector()
        corrected_deltas = self._apply_level_apparent_corrections(
            calculation_results, distances_mean_by_segment, corrector, atmospheric_conditions
        )
        print(f"   Corrections de niveaux apparents appliqu√©es")
        
        # 4. DESIGN MATRIX robuste selon votre logique (calcul√© en premier pour avoir les dimensions)
        A = self._build_robust_design_matrix(calculation_results)
        print(f"   Design matrix robuste construite: {A.shape}")
        
        # 3. MISCLOSURE VECTOR avec deltas corrig√©s (adapt√© aux dimensions de A)
        m_observations = A.shape[0]
        f = self._build_misclosure_vector_corrected(corrected_deltas, calculation_results, m_observations)
        print(f"   Misclosure vector calcul√©: {f.shape}")
        
        # 5. MATRICE DE POIDS avec distances moyennes (adapt√©e aux dimensions de A)
        m_observations = A.shape[0]  # Nombre d'observations = nombre de lignes de A
        P = self._build_weight_matrix_simplified(
            distances_mean_by_segment, instrumental_error_mm, kilometric_error_mm,
            m_observations
        )
        print(f"   Matrice de poids construite: {P.shape}")
        
        # 6. Identifiants (adapt√©s aux nouvelles dimensions)
        point_ids = [alt.point_id for alt in calculation_results.altitudes]
        observation_ids = [f"obs_{i+1}" for i in range(m_observations)]
        
        return MatrixSystem(
            A=A, P=P, f=f,
            point_ids=point_ids,
            observation_ids=observation_ids
        )
    
    def _calculate_mean_distances_by_segment(self, calculation_results: CalculationResults, 
                                           distances_m: np.ndarray, n_segments: int) -> np.ndarray:
        """1. Calcul de la moyenne des distances par segment: DM = (Ds1 + Ds2)/2."""
        mean_distances = np.zeros(n_segments)
        
        # Grouper les distances par segment selon les instruments
        # Chaque segment devrait avoir 2 distances (instrument 1 et 2)
        for seg in range(n_segments):
            # R√©cup√©rer les distances pour ce segment
            segment_distances = []
            
            # Si on a exactement 2*n_segments distances (2 par segment)
            if len(distances_m) == 2 * n_segments:
                ds1 = distances_m[seg * 2]      # Distance instrument 1
                ds2 = distances_m[seg * 2 + 1]  # Distance instrument 2
                segment_distances = [ds1, ds2]
            # Si on a n_segments distances (1 par segment)
            elif len(distances_m) == n_segments:
                if seg < len(distances_m):
                    # Utiliser la distance disponible comme moyenne
                    mean_distances[seg] = distances_m[seg]
                    continue
                else:
                    segment_distances = [100.0, 100.0]  # Distances par d√©faut
            else:
                # Essayer de r√©cup√©rer 2 distances par segment
                start_idx = seg * 2
                if start_idx < len(distances_m):
                    segment_distances.append(distances_m[start_idx])
                if start_idx + 1 < len(distances_m):
                    segment_distances.append(distances_m[start_idx + 1])
                
                # Compl√©ter avec distance par d√©faut si n√©cessaire
                while len(segment_distances) < 2:
                    segment_distances.append(100.0)
            
            # Appliquer la formule: DM = (Ds1 + Ds2) / 2
            if len(segment_distances) >= 2:
                mean_distances[seg] = (segment_distances[0] + segment_distances[1]) / 2.0
            else:
                mean_distances[seg] = segment_distances[0] if segment_distances else 100.0
        
        return mean_distances
    
    def _apply_level_apparent_corrections(self, calculation_results: CalculationResults,
                                        distances_mean: np.ndarray, 
                                        corrector, atmospheric_conditions) -> np.ndarray:
        """2. Application des corrections de niveaux apparents aux deltas moyens.
        
        Formule: correction = (DM¬≤/2R) + (-K * DM¬≤/2R)
        Application: deltacorrige = delta_moyen + correction_niveau_apparant
        """
        n_segments = len(distances_mean)
        corrected_deltas = np.zeros(n_segments)
        
        # Constantes
        EARTH_RADIUS = 6370000.0  # Rayon terrestre en m√®tres
        K_COEFFICIENT = 0.13      # Coefficient K standard
        
        # Obtenir les deltas moyens calcul√©s avec votre logique (delta_1 + delta_2) / 2
        mean_deltas = []
        if hasattr(calculation_results, 'height_differences'):
            current_segment = []
            
            for hd in calculation_results.height_differences:
                if hd.instrument_id == 1 and current_segment:
                    # Nouveau segment, traiter le pr√©c√©dent
                    if len(current_segment) >= 2:
                        mean_delta = (current_segment[0].delta_h_m + current_segment[1].delta_h_m) / 2.0
                    else:
                        mean_delta = current_segment[0].delta_h_m
                    mean_deltas.append(mean_delta)
                    current_segment = [hd]
                else:
                    current_segment.append(hd)
            
            # Traiter le dernier segment
            if current_segment:
                if len(current_segment) >= 2:
                    mean_delta = (current_segment[0].delta_h_m + current_segment[1].delta_h_m) / 2.0
                else:
                    mean_delta = current_segment[0].delta_h_m
                mean_deltas.append(mean_delta)
        
        # Appliquer votre formule de correction aux deltas moyens
        for i in range(n_segments):
            if i < len(mean_deltas):
                delta_moyen = mean_deltas[i]
                distance_moyenne = distances_mean[i]
                
                # Formule: correction = (DM¬≤/2R) + (-K * DM¬≤/2R)
                # Simplifi√©e: correction = DM¬≤ * (1 - K) / (2R)
                dm_squared = distance_moyenne ** 2
                terme1 = dm_squared / (2 * EARTH_RADIUS)          # DM¬≤/2R
                terme2 = -K_COEFFICIENT * dm_squared / (2 * EARTH_RADIUS)  # -K * DM¬≤/2R
                
                correction_niveau_apparant = terme1 + terme2
                
                # Application: deltacorrige = delta_moyen + correction_niveau_apparant
                corrected_deltas[i] = delta_moyen + correction_niveau_apparant
                
                print(f"   Segment {i+1}: DM={distance_moyenne:.1f}m, "
                      f"correction={correction_niveau_apparant*1000:.3f}mm, "
                      f"delta_moyen={delta_moyen*1000:.1f}mm ‚Üí "
                      f"delta_corrig√©={corrected_deltas[i]*1000:.1f}mm")
            else:
                corrected_deltas[i] = 0.0
        
        return corrected_deltas
    
    def _build_misclosure_vector_corrected(self, corrected_deltas: np.ndarray,
                                         calculation_results: CalculationResults,
                                         m_observations: int = None) -> np.ndarray:
        """3. Construction du misclosure vector avec les deltas corrig√©s.
        
        R√®gle: Pour tout point n'ayant pas de delta, le misclosure vector F = 0
               F = deltacorrige - (altitude(i) - altitude(i-1))
        """
        # Utiliser m_observations si fourni, sinon utiliser len(corrected_deltas)
        n_obs = m_observations if m_observations is not None else len(corrected_deltas)
        f = np.zeros(n_obs)
        
        # Identifier quels segments ont des observations valides
        segments_with_data = []
        if hasattr(calculation_results, 'height_differences'):
            # V√©rifier quels segments ont des donn√©es
            for i, hd in enumerate(calculation_results.height_differences):
                if hd.is_valid:
                    segment_idx = i // 2  # 2 instruments par segment
                    if segment_idx not in segments_with_data and segment_idx < n_segments:
                        segments_with_data.append(segment_idx)
        
        # Pour chaque observation
        for i in range(n_obs):
            if i < len(corrected_deltas) and i in segments_with_data and corrected_deltas[i] != 0:
                # Observation avec donn√©es: F = deltacorrige - (altitude(i) - altitude(i-1))
                if i+1 < len(calculation_results.altitudes):
                    altitude_from = calculation_results.altitudes[i].altitude_m
                    altitude_to = calculation_results.altitudes[i+1].altitude_m
                    computed_delta = altitude_to - altitude_from
                    
                    f[i] = corrected_deltas[i] - computed_delta
                    
                    print(f"   F[{i+1}] = {corrected_deltas[i]*1000:.1f} - {computed_delta*1000:.1f} = {f[i]*1000:.1f}mm")
                else:
                    f[i] = 0.0
            else:
                # Observation sans delta: F = 0
                f[i] = 0.0
                print(f"   F[{i+1}] = 0.0 (pas de delta)")
        
        return f.reshape(-1, 1)
    
    def _build_robust_design_matrix(self, calculation_results: CalculationResults) -> np.ndarray:
        """4. Construction de la design matrix robuste selon votre logique."""
        
        # Cr√©er un DataFrame fictif bas√© sur les altitudes pour avoir les matricules
        points_data = []
        for alt in calculation_results.altitudes:
            points_data.append({'Matricule': alt.point_id})
        
        df = pd.DataFrame(points_data)
        
        # Dimensions
        m = len(df) - 1           # number of obs = n_points - 1
        
        # Point de r√©f√©rence = premier point du dataframe (fix√©)
        reference_point = df.loc[0, "Matricule"]
        
        # Points inconnus = tous sauf le point de r√©f√©rence
        unknown_points = []
        point_to_index = {}
        
        for i, matricule in enumerate(df["Matricule"]):
            if matricule != reference_point:  # Exclure le point de r√©f√©rence
                unknown_points.append(matricule)
                # L'index dans la matrice A correspond √† l'ordre d'apparition (sans le point de r√©f√©rence)
                point_to_index[matricule] = len(point_to_index)
        
        n = len(unknown_points)   # number of unknowns
        
        print(f"   Design matrix: {m}√ó{n} (ref: {reference_point}, unknowns: {n})")
        
        # Construction de la matrice A
        A = np.zeros((m, n))
        
        for i in range(1, len(df)):  # Commencer √† 1 (skip reference point)
            p = df.loc[i-1, "Matricule"]  # Point de d√©part
            q = df.loc[i, "Matricule"]    # Point d'arriv√©e
            
            # Fill A row (i-1 car on commence √† i=1)
            if p in point_to_index:
                A[i-1, point_to_index[p]] = -1  # Point de d√©part
            if q in point_to_index:
                A[i-1, point_to_index[q]] = +1  # Point d'arriv√©e
        
        return A
    
    def _build_weight_matrix_simplified(self, distances_mean: np.ndarray,
                                      instrumental_error_mm: float,
                                      kilometric_error_mm: float,
                                      m_observations: int = None) -> np.ndarray:
        """5. Construction de la matrice de poids avec distances moyennes."""
        # Si m_observations est fourni, utiliser cette dimension pour la coh√©rence
        n_obs = m_observations if m_observations is not None else len(distances_mean)
        
        weights = np.zeros(n_obs)
        
        for i in range(n_obs):
            # Utiliser la distance correspondante ou la derni√®re disponible
            if i < len(distances_mean):
                dist_m = distances_mean[i]
            else:
                dist_m = distances_mean[-1] if len(distances_mean) > 0 else 100.0
            
            # Conversion en kilom√®tres
            dist_km = dist_m / 1000.0
            
            # Variance th√©orique: œÉ¬≤ = a¬≤ + b¬≤√ód (mm¬≤)
            variance_mm2 = instrumental_error_mm**2 + (kilometric_error_mm * dist_km)**2
            
            # Poids = 1/variance
            weights[i] = 1.0 / variance_mm2
        
        return np.diag(weights)
    
    def _calculate_direct_residuals(self, corrected_deltas: np.ndarray,
                                   adjusted_altitudes: List,
                                   calculation_results: CalculationResults) -> np.ndarray:
        """Calcul direct des r√©sidus selon votre formule.
        
        Formule: Residu = delta_corrige - (altitude_ajuste(i) - altitude_ajuste(i-1))
        """
        n_segments = len(corrected_deltas)
        residuals = np.zeros(n_segments)
        
        # Identifier quels segments ont des observations valides
        segments_with_data = []
        if hasattr(calculation_results, 'height_differences'):
            for i, hd in enumerate(calculation_results.height_differences):
                if hd.is_valid:
                    segment_idx = i // 2
                    if segment_idx not in segments_with_data and segment_idx < n_segments:
                        segments_with_data.append(segment_idx)
        
        # Calculer les r√©sidus pour chaque segment
        for i in range(n_segments):
            if i in segments_with_data and i < len(corrected_deltas):
                # R√©sidu = delta_corrig√© - (altitude_ajust√©e(i+1) - altitude_ajust√©e(i))
                altitude_ajuste_from = adjusted_altitudes[i].altitude_m
                altitude_ajuste_to = adjusted_altitudes[i+1].altitude_m
                delta_ajuste = altitude_ajuste_to - altitude_ajuste_from
                
                residuals[i] = corrected_deltas[i] - delta_ajuste
                
                print(f"   R√©sidu[{i+1}] = {corrected_deltas[i]*1000:.1f} - {delta_ajuste*1000:.1f} = {residuals[i]*1000:.3f}mm")
            else:
                residuals[i] = 0.0
        
        return residuals.reshape(-1, 1)
    
    def _get_corrected_deltas(self, calculation_results: CalculationResults,
                            distances_m: np.ndarray, atmospheric_conditions) -> np.ndarray:
        """M√©thode utilitaire pour r√©cup√©rer les deltas corrig√©s."""
        n_points = len(calculation_results.altitudes)
        n_segments = n_points - 1
        
        # 1. Moyenne des distances
        distances_mean_by_segment = self._calculate_mean_distances_by_segment(
            calculation_results, distances_m, n_segments
        )
        
        # 2. Application des corrections de niveaux apparents
        from atmospheric_corrections import AtmosphericCorrector
        corrector = AtmosphericCorrector()
        corrected_deltas = self._apply_level_apparent_corrections(
            calculation_results, distances_mean_by_segment, corrector, atmospheric_conditions
        )
        
        return corrected_deltas

    def build_complete_system(self, calculation_results: CalculationResults,
                             distances_m: np.ndarray,
                             instrumental_error_mm: float = 1.0,
                             kilometric_error_mm: float = 1.0) -> MatrixSystem:
        """
        Construction du syst√®me matriciel complet avec dimensions coh√©rentes.
        
        STRAT√âGIE: Utiliser TOUTES les observations individuelles (12)
        avec une matrice A qui mappe correctement aux segments.
        """
        n_points = len(calculation_results.altitudes)
        n_segments = n_points - 1
        
        print(f"üîß Build complete system:")
        print(f"   Points: {n_points}")
        print(f"   Segments: {n_segments}")
        
        # 1. TOUTES les observations valides
        observed_deltas = []
        observation_to_segment = []  # Mapping observation -> segment
        
        if calculation_results.height_differences:
            for i, hd in enumerate(calculation_results.height_differences):
                if hd.is_valid:
                    observed_deltas.append(hd.delta_h_m)
                    # D√©terminer quel segment : [0,1]->0, [2,3]->1, [4,5]->2, etc.
                    segment_idx = i // 2  # Division enti√®re
                    observation_to_segment.append(segment_idx)
        
        n_observations = len(observed_deltas)
        observed_deltas = np.array(observed_deltas)
        
        print(f"   Observations: {n_observations}")
        print(f"   Redondance: {n_observations - n_segments}")
        
        if n_observations <= n_segments:
            print("‚ö†Ô∏è Redondance insuffisante pour compensation par moindres carr√©s")
            print(f"   Solution: Fixer aussi le point final pour cr√©er de la redondance")
            print(f"   Actuellement: {n_observations} observations, {n_segments} segments")
            print(f"   Avec point final fix√©: {n_observations} observations, {n_segments-1} inconnues")
            
            # Pour cheminement ouvert: fixer les deux extr√©mit√©s
            # Cela r√©duit le nombre d'inconnues √† n_segments-1
            # et permet la compensation si n_observations > n_segments-1
            pass  # On garde toutes les observations mais on modifiera la matrice A
        
        # 2. Construction de la matrice A - adaptation pour cheminement ouvert
        if n_observations <= n_segments:
            # Pour cheminement ouvert: fixer les deux extr√©mit√©s
            n_unknowns = n_segments - 1  # Fixer premier ET dernier point
            print(f"   Matrice A: {n_observations} obs √ó {n_unknowns} inconnues (points 2 √† {n_segments})")
        else:
            # Cas normal avec redondance
            n_unknowns = n_segments
            print(f"   Matrice A: {n_observations} obs √ó {n_unknowns} inconnues")
        
        # Construction manuelle de la matrice A
        A = np.zeros((n_observations, n_unknowns))
        
        # 3. Remplir la matrice A selon le mapping observations->segments
        if len(observation_to_segment) == n_observations:
            for obs_idx in range(n_observations):
                segment_idx = observation_to_segment[obs_idx]
                
                if n_unknowns == n_segments - 1:
                    # Cheminement ouvert: points 1 et n fix√©s, inconnues = points 2 √† n-1
                    if 1 <= segment_idx <= n_unknowns:  # Segments concernent les points internes
                        A[obs_idx, segment_idx-1] = 1.0  # Point d'arriv√©e
                        if segment_idx > 1:
                            A[obs_idx, segment_idx-2] = -1.0  # Point de d√©part
                else:
                    # Cas normal
                    if segment_idx < n_unknowns:
                        A[obs_idx, segment_idx] = 1.0
                        if segment_idx > 0:
                            A[obs_idx, segment_idx-1] = -1.0
        
        # 4. Matrice de poids - une par observation
        if len(distances_m) != n_observations:
            # R√©p√©ter les distances par segment pour chaque observation
            distances_expanded = []
            for obs_idx in range(n_observations):
                segment_idx = observation_to_segment[obs_idx] if obs_idx < len(observation_to_segment) else obs_idx % n_segments
                if segment_idx < len(distances_m):
                    distances_expanded.append(distances_m[segment_idx])
                else:
                    distances_expanded.append(100.0)  # Distance par d√©faut
            distances_m = np.array(distances_expanded)
        
        P = self.build_weight_matrix(distances_m[:n_observations], instrumental_error_mm, kilometric_error_mm)
        
        # 5. D√©nivel√©es calcul√©es - une par observation
        computed_deltas = []
        for obs_idx in range(n_observations):
            segment_idx = observation_to_segment[obs_idx] if obs_idx < len(observation_to_segment) else obs_idx % n_segments
            if segment_idx < n_segments:
                computed_delta = (calculation_results.altitudes[segment_idx+1].altitude_m - 
                                calculation_results.altitudes[segment_idx].altitude_m)
                computed_deltas.append(computed_delta)
            else:
                computed_deltas.append(0.0)
        
        computed_deltas = np.array(computed_deltas)
        
        # 6. Vecteur des √©carts
        f = self.build_misclosure_vector(observed_deltas, computed_deltas)
        
        # 7. Identifiants
        point_ids = [alt.point_id for alt in calculation_results.altitudes]
        observation_ids = [f"obs_{i+1}" for i in range(n_observations)]
        
        print(f"   Matrices finales: A{A.shape}, P{P.shape}, f{f.shape}")
        
        return MatrixSystem(
            A=A, P=P, f=f,
            point_ids=point_ids,
            observation_ids=observation_ids
        )


def solve_least_squares_robust(A: np.ndarray, P: np.ndarray, f: np.ndarray) -> tuple:
    """
    R√©solution robuste pour donn√©es r√©elles avec matrices mal conditionn√©es.
    """
    n_obs, n_unknowns = A.shape
    
    # V√©rifier le conditionnement
    try:
        cond_number = np.linalg.cond(A)
        print(f"üîç Conditionnement matrice A: {cond_number:.2e}")
        
        if cond_number > 1e12:
            print("‚ö†Ô∏è Matrice mal conditionn√©e - utilisation SVD")
            return solve_svd_pseudoinverse(A, P, f)
        elif n_unknowns > 500:
            print("üîß Gros syst√®me - utilisation QR")
            return solve_qr_decomposition(A, P, f)
        else:
            print("üîß Syst√®me normal - √©quations normales")
            return solve_normal_equations_robust(A, P, f)
            
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur analyse - fallback SVD: {e}")
        return solve_svd_pseudoinverse(A, P, f)

def solve_svd_pseudoinverse(A: np.ndarray, P: np.ndarray, f: np.ndarray) -> tuple:
    """R√©solution par SVD pour matrices singuli√®res."""
    try:
        # Pond√©ration
        sqrt_P = np.sqrt(P)
        A_weighted = sqrt_P @ A
        f_weighted = sqrt_P @ f
        
        # SVD decomposition
        U, s, Vt = np.linalg.svd(A_weighted, full_matrices=False)
        
        # Pseudoinverse avec seuil pour valeurs singuli√®res
        s_threshold = np.max(s) * 1e-10
        s_inv = np.where(s > s_threshold, 1/s, 0)
        
        # Solution
        x_hat = Vt.T @ (s_inv[:, np.newaxis] * (U.T @ f_weighted))
        
        # Covariance approximative
        Qx = Vt.T @ np.diag(s_inv**2) @ Vt
        
        print(f"‚úÖ SVD: {np.sum(s > s_threshold)} valeurs singuli√®res retenues")
        
        return x_hat, Qx
        
    except Exception as e:
        raise MatrixError(f"√âchec SVD: {e}", operation="svd_solve")

def solve_qr_decomposition(A: np.ndarray, P: np.ndarray, f: np.ndarray) -> tuple:
    """R√©solution par d√©composition QR - plus stable pour matrices mal conditionn√©es."""
    try:
        # Pond√©ration
        sqrt_P = np.sqrt(P)
        A_weighted = sqrt_P @ A
        f_weighted = sqrt_P @ f
        
        # D√©composition QR
        Q, R = np.linalg.qr(A_weighted)
        
        # R√©solution du syst√®me triangulaire
        x_hat = scipy_linalg.solve_triangular(R, Q.T @ f_weighted)
        
        # Matrice de covariance via R
        R_inv = scipy_linalg.solve_triangular(R, np.eye(R.shape[0]))
        Qx = R_inv @ R_inv.T
        
        print(f"‚úÖ QR: matrice {A.shape} r√©solue")
        
        return x_hat, Qx
        
    except Exception as e:
        print(f"‚ö†Ô∏è QR d√©composition √©chou√©e: {e}")
        return solve_svd_pseudoinverse(A, P, f)

def solve_normal_equations_robust(A: np.ndarray, P: np.ndarray, f: np.ndarray) -> tuple:
    """√âquations normales avec r√©gularisation."""
    try:
        N = A.T @ P @ A
        b = A.T @ P @ f
        
        # Ajouter r√©gularisation si n√©cessaire
        cond_N = np.linalg.cond(N)
        if cond_N > 1e10:
            print(f"‚ö†Ô∏è R√©gularisation appliqu√©e (cond={cond_N:.2e})")
            regularization = np.trace(N) * 1e-12
            N += regularization * np.eye(N.shape[0])
        
        x_hat = np.linalg.solve(N, b)
        Qx = np.linalg.inv(N)
        
        return x_hat, Qx
        
    except Exception as e:
        print(f"‚ö†Ô∏è √âquations normales √©chou√©es: {e}")
        return solve_svd_pseudoinverse(A, P, f)

class LeastSquaresSolver:
    """
    Solveur par moindres carr√©s avec m√©thodes multiples.
    
    Impl√©mente diff√©rentes m√©thodes de r√©solution selon la taille
    et les caract√©ristiques du syst√®me.
    """
    
    def __init__(self, precision_mm: float = 2.0):
        self.precision_mm = precision_mm
        self.large_system_threshold = 1000  # Seuil pour gros syst√®mes
    
    def solve_system(self, matrix_system: MatrixSystem,
                    method: Optional[SolutionMethod] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        R√©solution du syst√®me par moindres carr√©s.
        
        S√©lection automatique de la m√©thode optimale:
        - Petits syst√®mes (n < 1000): √âquations normales
        - Gros syst√®mes: D√©composition QR
        - Syst√®mes tr√®s creux: Solveurs creux
        
        Args:
            matrix_system: Syst√®me matriciel
            method: M√©thode forc√©e (None = auto)
            
        Returns:
            Tuple[np.ndarray, np.ndarray]: (corrections, covariance)
        """
        A, P, f = matrix_system.A, matrix_system.P, matrix_system.f
        n_obs, n_unknowns = A.shape
        
        # S√©lection automatique de la m√©thode
        if method is None:
            method = self._select_optimal_method(A, P)
        
        try:
            # R√©solution selon la m√©thode
            if method == SolutionMethod.NORMAL_EQUATIONS:
                x_hat, Qx = solve_normal_equations_robust(A, P, f)
            elif method == SolutionMethod.QR_DECOMPOSITION:
                x_hat, Qx = solve_qr_decomposition(A, P, f)
            elif method == SolutionMethod.CHOLESKY:
                x_hat, Qx = solve_normal_equations_robust(A, P, f)
            else:
                raise CalculationError(
                    f"M√©thode non impl√©ment√©e: {method}",
                    calculation_type="least_squares"
                )
            
            # Validation du r√©sultat
            self._validate_solution(x_hat, A, f)
            
            return x_hat, Qx
            
        except np.linalg.LinAlgError as e:
            raise MatrixError(
                f"Erreur alg√©brique: {str(e)}",
                matrix_name="syst√®me",
                matrix_shape=A.shape,
                operation="r√©solution"
            )
        except Exception as e:
            raise CalculationError(
                f"Erreur r√©solution: {str(e)}",
                calculation_type="least_squares"
            )
    
    def _select_optimal_method(self, A: np.ndarray, P: np.ndarray) -> SolutionMethod:
        """S√©lection automatique de la m√©thode optimale."""
        n_obs, n_unknowns = A.shape
        
        # Analyse de la structure
        is_well_conditioned = np.linalg.cond(A) < 1e12
        is_small_system = n_unknowns < self.large_system_threshold
        
        if is_small_system and is_well_conditioned:
            return SolutionMethod.NORMAL_EQUATIONS
        elif is_well_conditioned:
            return SolutionMethod.CHOLESKY
        else:
            return SolutionMethod.QR_DECOMPOSITION
    
    def _solve_normal_equations(self, A: np.ndarray, P: np.ndarray, 
                               f: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        R√©solution par √©quations normales classiques.
        
        Algorithme:
        N = A^T P A
        b = A^T P f
        xÃÇ = N^(-1) b
        Q‚Çì = N^(-1)
        """
        # Formation des √©quations normales
        N = A.T @ P @ A  # Matrice normale
        b = A.T @ P @ f  # Second membre
        
        # V√©rification du conditionnement
        cond_number = np.linalg.cond(N)
        if cond_number > 1e12:
            raise MatrixError(
                f"Matrice mal conditionn√©e: {cond_number:.2e}",
                matrix_name="N",
                operation="conditionnement"
            )
        
        # R√©solution
        x_hat = np.linalg.solve(N, b)
        Qx = np.linalg.inv(N)  # Matrice de covariance
        
        return x_hat, Qx
    
    def _solve_qr_decomposition(self, A: np.ndarray, P: np.ndarray, 
                               f: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        R√©solution par d√©composition QR.
        
        Plus stable num√©riquement pour les syst√®mes mal conditionn√©s.
        
        Algorithme:
        A_weighted = ‚àöP A
        f_weighted = ‚àöP f
        Q, R = qr(A_weighted)
        xÃÇ = R^(-1) Q^T f_weighted
        """
        # Pond√©ration
        sqrt_P = np.sqrt(P)
        A_weighted = sqrt_P @ A
        f_weighted = sqrt_P @ f
        
        # D√©composition QR
        Q, R = np.linalg.qr(A_weighted)
        
        # R√©solution du syst√®me triangulaire
        x_hat = scipy_linalg.solve_triangular(R, Q.T @ f_weighted)
        
        # Matrice de covariance via R
        R_inv = scipy_linalg.solve_triangular(R, np.eye(R.shape[0]))
        Qx = R_inv @ R_inv.T
        
        return x_hat, Qx
    
    def _solve_cholesky(self, A: np.ndarray, P: np.ndarray, 
                       f: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """
        R√©solution par d√©composition de Cholesky.
        
        Optimis√© pour matrices sym√©triques d√©finies positives.
        """
        # Formation des √©quations normales
        N = A.T @ P @ A
        b = A.T @ P @ f
        
        # D√©composition de Cholesky
        try:
            L = np.linalg.cholesky(N)
        except np.linalg.LinAlgError:
            # Fallback vers √©quations normales si Cholesky √©choue
            return self._solve_normal_equations(A, P, f)
        
        # R√©solution par substitution
        y = scipy_linalg.solve_triangular(L, b, lower=True)
        x_hat = scipy_linalg.solve_triangular(L.T, y, lower=False)
        
        # Matrice de covariance
        L_inv = scipy_linalg.solve_triangular(L, np.eye(L.shape[0]), lower=True)
        Qx = L_inv.T @ L_inv
        
        return x_hat, Qx
    
    def _validate_solution(self, x_hat: np.ndarray, A: np.ndarray, f: np.ndarray):
        """Validation de la solution obtenue."""
        # V√©rifier que la solution est finie
        if not np.all(np.isfinite(x_hat)):
            raise CalculationError(
                "Solution contient des valeurs infinies ou NaN",
                calculation_type="validation_solution"
            )
        
        # V√©rifier l'ordre de grandeur des corrections
        max_correction_mm = np.max(np.abs(x_hat)) * 1000
        
        # Pour maintenir pr√©cision 2mm, on accepte des corrections importantes
        # mais on les signale pour investigation
        extreme_threshold_mm = 10000  # 1cm = seuil d'erreur grave
        
        if max_correction_mm > extreme_threshold_mm:
            raise PrecisionError(
                f"Corrections extr√™mes d√©tect√©es: {max_correction_mm:.1f}mm - donn√©es probablement corrompues",
                measured_error=max_correction_mm,
                precision_mm=self.precision_mm
            )
        elif max_correction_mm > 1000:  # > 1m
            print(f"üîç CORRECTIONS IMPORTANTES: {max_correction_mm:.1f}mm")
            print(f"   Cible pr√©cision: {self.precision_mm}mm")
            print(f"   ‚ö†Ô∏è V√©rifiez imp√©rativement:")
            print(f"      ‚Ä¢ Altitudes de r√©f√©rence (initiale/finale)")
            print(f"      ‚Ä¢ Coh√©rence des unit√©s dans les donn√©es") 
            print(f"      ‚Ä¢ Qualit√© des observations AR/AV")
        elif max_correction_mm > 100:  # > 10cm
            print(f"‚ö†Ô∏è Corrections: {max_correction_mm:.1f}mm (> 10cm) - contr√¥lez les donn√©es")
    
    def _validate_solution_old(self, x_hat: np.ndarray, A: np.ndarray, f: np.ndarray):
        """Validation de la solution - d√©l√®gue au CompensationValidator."""
        validator = CompensationValidator(self.precision_mm)
        result = validator.validate_solution(x_hat, A, f)
        
        if not result.is_valid:
            raise PrecisionError(
                f"Solution invalide: {'; '.join(result.errors)}",
                measured_error=result.details.get('max_correction_mm', 0),
                precision_mm=self.precision_mm
            )
        
        # Afficher les avertissements
        for warning in result.warnings:
            print(f"‚ö†Ô∏è {warning}")


class StatisticalAnalyzer:
    """
    Analyseur statistique pour la compensation.
    
    Calcule les statistiques de qualit√© et d√©tecte les fautes grossi√®res
    selon la th√©orie statistique des moindres carr√©s.
    """
    
    def __init__(self, confidence_level: float = 0.95):
        self.confidence_level = confidence_level
        self.alpha = 1 - confidence_level
    
    def analyze_compensation(self, matrix_system: MatrixSystem,
                           x_hat: np.ndarray, 
                           Qx: np.ndarray) -> CompensationStatistics:
        """
        Analyse statistique compl√®te de la compensation.
        
        Calcule:
        - √âcart-type a posteriori œÉ‚ÇÄ
        - Test du poids unitaire (œá¬≤)
        - R√©sidus normalis√©s
        - D√©tection des fautes grossi√®res
        
        Args:
            matrix_system: Syst√®me matriciel
            x_hat: Corrections estim√©es
            Qx: Matrice de covariance
            
        Returns:
            CompensationStatistics: Statistiques compl√®tes
        """
        A, P, f = matrix_system.A, matrix_system.P, matrix_system.f
        n_obs, n_unknowns = A.shape
        
        # Calcul des r√©sidus
        v = A @ x_hat - f  # R√©sidus des observations
        
        # Degr√©s de libert√©
        r = n_obs - n_unknowns
        
        if r <= 0:
            raise CalculationError(
                f"Syst√®me sous-d√©termin√©: {n_obs} observations, {n_unknowns} inconnues",
                calculation_type="statistical_analysis"
            )
        
        # √âcart-type a posteriori
        vtPv = (v.T @ P @ v)[0, 0]  # Forme quadratique
        sigma_0_hat = np.sqrt(vtPv / r)
        
        # Test du œá¬≤ pour le poids unitaire
        chi2_statistic = vtPv
        from scipy.stats import chi2
        chi2_critical = chi2.ppf(self.confidence_level, r)
        unit_weight_valid = chi2_statistic <= chi2_critical
        
        # R√©sidus normalis√©s pour d√©tection de fautes
        normalized_residuals = self._calculate_normalized_residuals(v, A, P, Qx, sigma_0_hat)
        max_normalized_residual = np.max(np.abs(normalized_residuals))
        
        # Seuil de d√©tection des fautes (test de Student)
        from scipy.stats import t
        t_critical = t.ppf(1 - self.alpha/2, r)  # Test bilat√©ral
        blunder_threshold = t_critical
        
        return CompensationStatistics(
            sigma_0_hat=sigma_0_hat,
            degrees_of_freedom=r,
            chi2_test_statistic=chi2_statistic,
            chi2_critical_value=chi2_critical,
            unit_weight_valid=unit_weight_valid,
            max_standardized_residual=max_normalized_residual,
            blunder_detection_threshold=blunder_threshold
        )
    
    def _calculate_normalized_residuals(self, v: np.ndarray, A: np.ndarray, 
                                      P: np.ndarray, Qx: np.ndarray, 
                                      sigma_0: float) -> np.ndarray:
        """
        Calcul des r√©sidus normalis√©s.
        
        Th√©orie:
        rÃÇ·µ¢ = v·µ¢ / (œÉ‚ÇÄ ‚àöq·µ•·µ•·µ¢)
        o√π q·µ•·µ•·µ¢ = (P‚Åª¬π - A Q‚Çì A·µÄ)·µ¢·µ¢
        """
        n_obs = len(v)
        P_inv = np.linalg.inv(P)
        
        # Matrice de covariance des r√©sidus
        Qv = P_inv - A @ Qx @ A.T
        
        # R√©sidus normalis√©s
        normalized_residuals = np.zeros(n_obs)
        for i in range(n_obs):
            if Qv[i, i] > 0:  # √âviter division par z√©ro
                normalized_residuals[i] = v[i, 0] / (sigma_0 * np.sqrt(Qv[i, i]))
            else:
                normalized_residuals[i] = 0.0
        
        return normalized_residuals
    
    def detect_blunders(self, normalized_residuals: np.ndarray,
                       threshold: float,
                       observation_ids: List[str]) -> Dict:
        """
        D√©tection des fautes grossi√®res.
        
        Args:
            normalized_residuals: R√©sidus normalis√©s
            threshold: Seuil de d√©tection
            observation_ids: Identifiants des observations
            
        Returns:
            Dict: Rapport de d√©tection des fautes
        """
        suspect_observations = []
        
        for i, (residual, obs_id) in enumerate(zip(normalized_residuals, observation_ids)):
            if abs(residual) > threshold:
                suspect_observations.append({
                    'observation_id': obs_id,
                    'index': i,
                    'normalized_residual': residual,
                    'significance': abs(residual) / threshold
                })
        
        return {
            'total_observations': len(normalized_residuals),
            'suspect_count': len(suspect_observations),
            'suspect_observations': suspect_observations,
            'detection_threshold': threshold,
            'max_residual': np.max(np.abs(normalized_residuals)),
            'blunders_detected': len(suspect_observations) > 0
        }


class LevelingCompensator:
    """
    Compensateur principal pour nivellement g√©om√©trique.
    
    Orchestre l'ensemble du processus de compensation par moindres carr√©s
    avec validation statistique et contr√¥le de pr√©cision 2mm.
    """
    
    def __init__(self, precision_mm: float = 2.0,
                 instrumental_error_mm: float = 1.0,
                 kilometric_error_mm: float = 1.0,
                 confidence_level: float = 0.95):
        """
        Args:
            precision_mm: Pr√©cision cible (mm)
            instrumental_error_mm: Erreur instrumentale (mm)
            kilometric_error_mm: Erreur kilom√©trique (mm/km)
            confidence_level: Niveau de confiance statistique
        """
        self.precision_mm = precision_mm
        self.instrumental_error_mm = instrumental_error_mm
        self.kilometric_error_mm = kilometric_error_mm
        
        # Modules
        self.matrix_builder = MatrixBuilder()
        self.solver = LeastSquaresSolver(precision_mm)
        self.statistical_analyzer = StatisticalAnalyzer(confidence_level)
        self.precision_validator = PrecisionValidator(precision_mm)
    
    def compensate_restructured(self, calculation_results: CalculationResults,
                              distances_m: np.ndarray,
                              atmospheric_conditions,
                              solution_method: Optional[SolutionMethod] = None) -> CompensationResults:
        """
        Compensation compl√®te avec le nouveau processus restructur√©.
        
        Processus:
        1. Moyenne des distances par segment  
        2. Correction des niveaux apparents (courbure + r√©fraction)
        3. Application corrections aux deltas moyens
        4. Misclosure vector
        5. Design matrix
        6. Matrice de poids
        7. R√©solution et analyse
        """
        try:
            # 1-6. Construction du syst√®me matriciel avec nouveau processus
            matrix_system = self.matrix_builder.build_complete_system_restructured(
                calculation_results, distances_m, atmospheric_conditions,
                self.instrumental_error_mm, self.kilometric_error_mm
            )
            
            # R√©cup√©rer les deltas corrig√©s pour le calcul des r√©sidus
            corrected_deltas = self.matrix_builder._get_corrected_deltas(
                calculation_results, distances_m, atmospheric_conditions
            )
            
            # 7. R√©solution par moindres carr√©s
            x_hat, Qx = self.solver.solve_system(matrix_system, solution_method)
            
            # 8. Calcul des altitudes compens√©es
            adjusted_altitudes = self._calculate_adjusted_altitudes(
                calculation_results.altitudes, x_hat
            )
            
            # 9. Calcul des r√©sidus selon votre formule directe
            print("üßÆ Calcul des r√©sidus selon formule directe:")
            residuals = self.matrix_builder._calculate_direct_residuals(
                corrected_deltas, adjusted_altitudes, calculation_results
            )
            
            # 10. Analyse statistique avec r√©sidus matriciels pour compatibilit√©
            matrix_residuals = matrix_system.A @ x_hat - matrix_system.f
            statistics = self.statistical_analyzer.analyze_compensation(
                matrix_system, x_hat, Qx
            )
            
            # 11. M√©tadonn√©es
            metadata = {
                'compensation_timestamp': pd.Timestamp.now(),
                'method_used': solution_method or self.solver._select_optimal_method(
                    matrix_system.A, matrix_system.P
                ),
                'system_size': f"{matrix_system.A.shape[0]}x{matrix_system.A.shape[1]}",
                'condition_number': np.linalg.cond(matrix_system.A),
                'max_correction_mm': np.max(np.abs(x_hat)) * 1000,
                'precision_target_mm': self.precision_mm,
                'process_type': 'restructured_with_level_apparent_corrections'
            }
            
            # 12. D√©tection des fautes grossi√®res
            normalized_residuals = self.statistical_analyzer._calculate_normalized_residuals(
                residuals, matrix_system.A, matrix_system.P, Qx, statistics.sigma_0_hat
            )
            
            blunder_report = self.statistical_analyzer.detect_blunders(
                normalized_residuals, statistics.blunder_detection_threshold,
                matrix_system.observation_ids
            )
            metadata['blunder_detection'] = blunder_report
            
            return CompensationResults(
                adjusted_coordinates=x_hat,
                adjusted_altitudes=adjusted_altitudes,
                residuals=residuals,
                covariance_matrix=Qx,
                statistics=statistics,
                solution_method=metadata['method_used'],
                computation_metadata=metadata
            )
            
        except Exception as e:
            raise CalculationError(
                f"Erreur compensation restructur√©e: {str(e)}",
                calculation_type="compensation_restructured"
            )

    def compensate(self, calculation_results: CalculationResults,
                  distances_m: np.ndarray,
                  solution_method: Optional[SolutionMethod] = None) -> CompensationResults:
        """
        Compensation compl√®te par moindres carr√©s.
        
        Pipeline complet:
        1. Construction du syst√®me matriciel
        2. R√©solution par moindres carr√©s
        3. Calcul des altitudes compens√©es
        4. Analyse statistique
        5. Validation de pr√©cision
        
        Args:
            calculation_results: R√©sultats des calculs pr√©liminaires
            distances_m: Distances des observations
            solution_method: M√©thode de r√©solution (None = auto)
            
        Returns:
            CompensationResults: R√©sultats complets de compensation
        """
        try:
            # 1. Construction du syst√®me matriciel
            matrix_system = self.matrix_builder.build_complete_system(
                calculation_results, distances_m,
                self.instrumental_error_mm, self.kilometric_error_mm
            )
            
            # 2. R√©solution par moindres carr√©s
            x_hat, Qx = self.solver.solve_system(matrix_system, solution_method)
            
            # 3. Calcul des altitudes compens√©es
            adjusted_altitudes = self._calculate_adjusted_altitudes(
                calculation_results.altitudes, x_hat
            )
            
            # 4. Calcul des r√©sidus
            residuals = matrix_system.A @ x_hat - matrix_system.f
            
            # 5. Analyse statistique
            statistics = self.statistical_analyzer.analyze_compensation(
                matrix_system, x_hat, Qx
            )
            
            # 6. M√©tadonn√©es
            metadata = {
                'compensation_timestamp': pd.Timestamp.now(),
                'method_used': solution_method or self.solver._select_optimal_method(
                    matrix_system.A, matrix_system.P
                ),
                'system_size': f"{matrix_system.A.shape[0]}x{matrix_system.A.shape[1]}",
                'condition_number': np.linalg.cond(matrix_system.A),
                'max_correction_mm': np.max(np.abs(x_hat)) * 1000,
                'precision_target_mm': self.precision_mm
            }
            
            # 7. D√©tection des fautes grossi√®res
            normalized_residuals = self.statistical_analyzer._calculate_normalized_residuals(
                residuals, matrix_system.A, matrix_system.P, Qx, statistics.sigma_0_hat
            )
            
            blunder_report = self.statistical_analyzer.detect_blunders(
                normalized_residuals, statistics.blunder_detection_threshold,
                matrix_system.observation_ids
            )
            metadata['blunder_detection'] = blunder_report
            
            return CompensationResults(
                adjusted_coordinates=x_hat,
                adjusted_altitudes=adjusted_altitudes,
                residuals=residuals,
                covariance_matrix=Qx,
                statistics=statistics,
                solution_method=metadata['method_used'],
                computation_metadata=metadata
            )
            
        except Exception as e:
            raise CalculationError(
                f"Erreur compensation: {str(e)}",
                calculation_type="compensation_complete"
            )
    
    def _calculate_adjusted_altitudes(self, original_altitudes: List[AltitudeCalculation],
                                    corrections: np.ndarray) -> List[AltitudeCalculation]:
        """Calcul des altitudes compens√©es."""
        adjusted_altitudes = []
        n_corrections = corrections.shape[0]
        n_points = len(original_altitudes)
        
        for i, original_alt in enumerate(original_altitudes):
            if i == 0:  # Premier point fixe (r√©f√©rence)
                adjusted_alt = AltitudeCalculation(
                    point_id=original_alt.point_id,
                    altitude_m=original_alt.altitude_m,
                    cumulative_delta_h=0.0,
                    is_reference=True
                )
            elif i == n_points - 1 and n_corrections == n_points - 2:  # Dernier point fixe (cheminement ouvert)
                adjusted_alt = AltitudeCalculation(
                    point_id=original_alt.point_id,
                    altitude_m=original_alt.altitude_m,  # Altitude finale fix√©e
                    cumulative_delta_h=original_alt.cumulative_delta_h,
                    is_reference=True
                )
            else:  # Points interm√©diaires corrig√©s
                correction_index = i - 1 if n_corrections == n_points - 1 else i - 1
                if correction_index < n_corrections:
                    correction = corrections[correction_index, 0]  # Correction en m√®tres
                    adjusted_altitude = original_alt.altitude_m + correction
                else:
                    adjusted_altitude = original_alt.altitude_m  # Pas de correction disponible
                
                adjusted_alt = AltitudeCalculation(
                    point_id=original_alt.point_id,
                    altitude_m=round(adjusted_altitude, 4),  # Pr√©cision 0.1mm
                    cumulative_delta_h=original_alt.cumulative_delta_h,
                    is_reference=False
                )
            
            adjusted_altitudes.append(adjusted_alt)
        
        return adjusted_altitudes
    
    def validate_compensation_quality(self, results: CompensationResults) -> ValidationResult:
        """
        Validation compl√®te de la qualit√© de compensation.
        
        Pour pr√©cision cible 2mm:
        - Accepte des corrections importantes si statistiquement coh√©rentes
        - Se concentre sur la qualit√© finale plut√¥t que les corrections interm√©diaires
        - Fournit diagnostics d√©taill√©s
        """
        validation = ValidationResult(True, [], [], {})
        
        # 1. Analyse des corrections importantes
        max_correction_mm = np.max(np.abs(results.adjusted_coordinates)) * 1000
        
        # Pour pr√©cision 2mm, on accepte des corrections importantes avec diagnostic
        if max_correction_mm > 1000:  # >1m
            validation.add_warning(
                f"Corrections tr√®s importantes: {max_correction_mm:.1f}mm"
            )
            validation.details['correction_analysis'] = self._analyze_large_corrections(results)
        elif max_correction_mm > 100:  # >10cm
            validation.add_info(
                f"Corrections notables: {max_correction_mm:.1f}mm"
            )
        
        # 2. Validation statistique (crit√®re principal pour 2mm)
        stats = results.statistics
        if not stats.unit_weight_valid:
            validation.add_warning(
                f"Test œá¬≤ √©chou√©: {stats.chi2_test_statistic:.2f} > {stats.chi2_critical_value:.2f}"
            )
        
        # 3. D√©tection fautes grossi√®res
        blunder_info = results.computation_metadata.get('blunder_detection', {})
        if blunder_info.get('blunders_detected', False):
            validation.add_error(
                f"Fautes d√©tect√©es: {blunder_info['suspect_count']} observations suspectes"
            )
        
        # 4. Validation r√©sidus
        if stats.max_standardized_residual > 3.0:  # Seuil 3-sigma
            validation.add_warning(
                f"R√©sidu normalis√© √©lev√©: {stats.max_standardized_residual:.2f}"
            )
        
        # 5. Validation pr√©cision finale atteinte
        final_precision_achieved = self._estimate_final_precision(results)
        if final_precision_achieved <= self.precision_mm:
            validation.add_success(
                f"Pr√©cision cible atteinte: {final_precision_achieved:.2f}mm ‚â§ {self.precision_mm}mm"
            )
        else:
            validation.add_warning(
                f"Pr√©cision finale: {final_precision_achieved:.2f}mm > {self.precision_mm}mm"
            )
        
        # 6. D√©tails validation
        validation.details = {
            'max_correction_mm': max_correction_mm,
            'final_precision_mm': final_precision_achieved,
            'sigma_0_posteriori': stats.sigma_0_hat,
            'degrees_of_freedom': stats.degrees_of_freedom,
            'chi2_valid': stats.unit_weight_valid,
            'max_normalized_residual': stats.max_standardized_residual,
            'blunders_detected': blunder_info.get('suspect_count', 0),
            'precision_target_mm': self.precision_mm
        }
        
        return validation
    
    def _analyze_large_corrections(self, results: CompensationResults) -> Dict:
        """Analyse des corrections importantes pour diagnostic."""
        analysis = {
            'corrections_mm': (results.adjusted_coordinates * 1000).flatten(),
            'correction_pattern': 'systematic' if self._is_systematic_correction(results) else 'random',
            'likely_causes': []
        }
        
        # D√©tection des causes probables
        corrections = results.adjusted_coordinates.flatten()
        if len(corrections) > 1:
            if np.std(corrections) < np.mean(np.abs(corrections)) * 0.1:
                analysis['likely_causes'].append("Erreur syst√©matique d'altitude de r√©f√©rence")
            if np.any(np.diff(corrections) > 0.01):  # >1cm de variation
                analysis['likely_causes'].append("Erreurs dans les observations individuelles")
        
        return analysis
    
    def _is_systematic_correction(self, results: CompensationResults) -> bool:
        """D√©tecte si les corrections suivent un pattern syst√©matique."""
        corrections = results.adjusted_coordinates.flatten()
        if len(corrections) < 3:
            return False
        
        # Teste la lin√©arit√© des corrections
        x = np.arange(len(corrections))
        correlation = np.corrcoef(x, corrections)[0, 1]
        return abs(correlation) > 0.8
    
    def _estimate_final_precision(self, results: CompensationResults) -> float:
        """Estime la pr√©cision finale atteinte apr√®s compensation."""
        # Utilise l'√©cart-type a posteriori et la matrice de covariance
        stats = results.statistics
        max_std_mm = np.sqrt(np.max(np.diag(results.covariance_matrix))) * 1000
        
        # Pr√©cision finale = combinaison de l'√©cart-type a posteriori et covariance max
        return max(stats.sigma_0_hat * 1000, max_std_mm)
    
    def generate_compensation_report(self, results: CompensationResults) -> str:
        """G√©n√®re un rapport d√©taill√© de compensation."""
        stats = results.statistics
        metadata = results.computation_metadata
        
        report = f"""
{'='*70}
    RAPPORT DE COMPENSATION - MOINDRES CARR√âS
{'='*70}

üéØ OBJECTIF PR√âCISION: {self.precision_mm} mm

üìä SYST√àME MATRICIEL:
   Taille: {metadata['system_size']}
   M√©thode: {metadata['method_used'].value}
   Conditionnement: {metadata['condition_number']:.2e}
   Correction max: {metadata['max_correction_mm']:.2f} mm

üìà STATISTIQUES QUALIT√â:
   œÉ‚ÇÄ (a posteriori): {stats.sigma_0_hat:.4f}
   Degr√©s libert√©: {stats.degrees_of_freedom}
   Test œá¬≤ (poids): {stats.chi2_test_statistic:.2f} / {stats.chi2_critical_value:.2f}
   Statut œá¬≤: {'‚úÖ VALID√â' if stats.unit_weight_valid else '‚ùå REJET√â'}

üîç D√âTECTION FAUTES:
   R√©sidu normalis√© max: {stats.max_standardized_residual:.2f}
   Seuil d√©tection: {stats.blunder_detection_threshold:.2f}
   Observations suspectes: {metadata['blunder_detection']['suspect_count']}

‚úÖ ALTITUDES COMPENS√âES:
   R√©f√©rence: {results.adjusted_altitudes[0].altitude_m:.4f} m
   Finale: {results.adjusted_altitudes[-1].altitude_m:.4f} m
   Points trait√©s: {len(results.adjusted_altitudes)}

‚è±Ô∏è M√âTADONN√âES:
   Timestamp: {metadata['compensation_timestamp']}
   Erreur instrumentale: {self.instrumental_error_mm} mm
   Erreur kilom√©trique: {self.kilometric_error_mm} mm/km

{'='*70}
"""
        
        return report
    
    def export_results_to_dataframe(self, results: CompensationResults) -> pd.DataFrame:
        """Export des r√©sultats compens√©s vers DataFrame."""
        data = []
        
        n_corrections = results.adjusted_coordinates.shape[0]
        n_points = len(results.adjusted_altitudes)
        
        for i, altitude in enumerate(results.adjusted_altitudes):
            # D√©terminer si ce point a une correction
            has_correction = False
            correction_value = 0.0
            
            if n_corrections == n_points - 1:  # Cas normal (premier point fixe)
                if i > 0:
                    correction_value = results.adjusted_coordinates[i-1, 0]
                    has_correction = True
            elif n_corrections == n_points - 2:  # Cheminement ouvert (deux points fixes)
                if 0 < i < n_points - 1:
                    correction_value = results.adjusted_coordinates[i-1, 0]
                    has_correction = True
            
            row = {
                'Matricule': altitude.point_id,
                'Altitude_originale': altitude.altitude_m - correction_value,
                'Correction_m': correction_value,
                'Correction_mm': correction_value * 1000,
                'Altitude_compens√©e': altitude.altitude_m,
                'Est_r√©f√©rence': altitude.is_reference,
                '√âcart_type_mm': (np.sqrt(results.covariance_matrix[i-1, i-1]) * 1000 
                                if has_correction and i-1 < results.covariance_matrix.shape[0] else 0.0)
            }
            
            # Ajouter r√©sidu si disponible
            if i < len(results.residuals):
                row['R√©sidu_mm'] = results.residuals[i, 0] * 1000
            
            data.append(row)
        
        return pd.DataFrame(data)